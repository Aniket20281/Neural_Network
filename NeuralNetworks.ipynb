{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural-Network Implementation from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemented a neural network for XOR gate. It will be a neural network with a hidden layer in it. The hidden layer will have two units in it. I started by choosing some random weights and biases and find out the predicted y for these random values. \n",
    "\n",
    "Then we will use gradient descent technique to reduce the cost function and then we will back propagate the values and update weights and biases multiple times till we reach a good enough output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of sigmoid function\n",
    "\n",
    "def derivative_sigmoid(z):\n",
    "    return (sigmoid(z)*(1 - sigmoid(z))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 2), (4, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input values\n",
    "X = np.array([[0,0] , [0,1] , [1,0] , [1,1]])\n",
    "# output values\n",
    "Y = np.array([[0] , [1] , [1] , [0]])\n",
    "X.shape , Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The below function will generate random weigths and biases for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which generates numpy array random number of size n X m\n",
    "\n",
    "def generate_random(n,m):\n",
    "    return ((np.random.random((n,m))))\n",
    "\n",
    "def bring_in_range(x):\n",
    "    return 2*x - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating random weigths and biases for every layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating random weights and biases\n",
    "\n",
    "weights_hidden = generate_random(2,2)\n",
    "weights_hidden = bring_in_range(weights_hidden)\n",
    "bias_hidden = generate_random(1,2)\n",
    "bias_hidden = bring_in_range(bias_hidden)\n",
    "weights_output = generate_random(2,1)\n",
    "weights_output = bring_in_range(weights_output)\n",
    "bias_output = generate_random(1,1)\n",
    "bias_output = bring_in_range(bias_output)\n",
    "\n",
    "# Defining a learning rate for gradient decent, used for cost function minimization\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FORWARD PROPAGATION (Without back_propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65698497],\n",
       "       [0.64178181],\n",
       "       [0.64981579],\n",
       "       [0.63743885]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_output = X\n",
    "\n",
    "# preparing the input for the hidden layer by multiplying weigths and adding biases\n",
    "hidden_layer_input = np.dot(X , weights_hidden) + bias_hidden\n",
    "\n",
    "# applying sigmoid function on the input\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "# preparing the input for the output layer by multiplying weigths and adding biases\n",
    "output_layer_input = np.dot(hidden_layer_output , weights_output) + bias_output\n",
    "\n",
    "# applying sigmoid function on the input\n",
    "final_output = sigmoid(output_layer_input)\n",
    "\n",
    "final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have received an output but not good. The expected output was [0,1,1,0] and this is not even close. This happened as we didn't apply back propagation to minimize the error. This output is just for random weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation optimized\n",
    "\n",
    "# We will perform 10,000 iterations to reduce error\n",
    "for iterate in range(10000):\n",
    "    expected_output = X\n",
    "    \n",
    "    # FORWARD PROPAGATION STARTS HERE\n",
    "    \n",
    "    # preparing the input for the hidden layer by multiplying weigths and adding biases\n",
    "    hidden_layer_input = np.dot(expected_output , weights_hidden) + bias_hidden\n",
    "    \n",
    "    # applying sigmoid function on the input\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    \n",
    "    # preparing the input for the output layer by multiplying weigths and adding biases\n",
    "    output_layer_input = np.dot(hidden_layer_output , weights_output) + bias_output\n",
    "    \n",
    "    # applying sigmoid function on the input\n",
    "    final_output = sigmoid(output_layer_input)\n",
    "    \n",
    "    # FORWARD PROPAGATION ENDS HERE\n",
    "    \n",
    "    # BACKWARD PROPAGATION STARTS HERE\n",
    "\n",
    "    # output_layer_first_term = Y predicted - Y actual\n",
    "    output_layer_first_term = final_output - Y\n",
    "\n",
    "    # output_layer_second_term = derivative of the sigmoid (input for the layer)\n",
    "    output_layer_second_term = derivative_sigmoid(output_layer_input)\n",
    "\n",
    "    output_layer_first_two = output_layer_second_term * output_layer_first_term\n",
    "\n",
    "    hidden_layer_first_term = np.dot(output_layer_first_two , weights_output.T)\n",
    "    hidden_layer_second_term = derivative_sigmoid(hidden_layer_input)\n",
    "    hidden_layer_first_two = hidden_layer_second_term * hidden_layer_first_term\n",
    "    \n",
    "    # Calculating the changes in weights required\n",
    "\n",
    "    changes_output_weight = np.dot(hidden_layer_output.T , output_layer_first_two)\n",
    "    changes_output_bias = np.sum(output_layer_first_two , axis = 0 , keepdims = True )\n",
    "    # If keepdims will be false then this will return us a single number but we want an array\n",
    "\n",
    "    changes_hidden_weight = np.dot(expected_output.T , hidden_layer_first_two)\n",
    "    changes_hidden_bias = np.sum(hidden_layer_first_two , axis = 0 , keepdims = True )\n",
    "    \n",
    "    # Updating weights and biases\n",
    "\n",
    "    weights_output = weights_output - learning_rate * changes_output_weight\n",
    "    bias_output = bias_output - learning_rate * changes_output_bias\n",
    "    weights_hidden = weights_hidden - learning_rate * changes_hidden_weight\n",
    "    bias_hidden = bias_hidden - learning_rate * changes_hidden_bias\n",
    "    \n",
    "    # BACKWARD PROPAGATION ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have the updated weights and biases so we calculate the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have updated the weights, so we will find the final output with updated weights\n",
    "expected_output = X\n",
    "hidden_layer_input = np.dot(X , weights_hidden) + bias_hidden\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "output_layer_input = np.dot(hidden_layer_output , weights_output) + bias_output\n",
    "final_output = sigmoid(output_layer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05514049],\n",
       "       [0.93250801],\n",
       "       [0.93231645],\n",
       "       [0.06878953]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output\n",
    "\n",
    "# We can clearly see that the final_output is very close to [0,1,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After applying the forward and the back propagation over multiple iterations, we have reached a final output which is extremely close to the required output [0,1,1,0]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
